---
title: "Data 621 HW#3"
author: "Group 3"
date: "2024-03-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Logistic Regression of Crime Data

## Training-Data Exploration

```{r}
library(dplyr)
library(ggplot2)
library(gt)
library(tidyr)
```

### Summary Stats

```{r}

train_dat=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw3/main/crime-training-data_modified.csv"))

summary_stats = train_dat %>%
  summarise(
    Mean_ZN = mean(zn, na.rm = TRUE),
    SD_ZN = sd(zn, na.rm = TRUE),
    Median_ZN = median(zn, na.rm = TRUE),
    Mean_INDUS = mean(indus, na.rm = TRUE),
    SD_INDUS = sd(indus, na.rm = TRUE),
    Median_INDUS = median(indus, na.rm = TRUE),
    Mean_CHAS = mean(chas, na.rm = TRUE),
    SD_CHAS = sd(chas, na.rm = TRUE),
    Median_CHAS = median(chas, na.rm = TRUE),
    Mean_NOX = mean(nox, na.rm = TRUE),
    SD_NOX = sd(nox, na.rm = TRUE),
    Median_NOX = median(nox, na.rm = TRUE),
    Mean_RM = mean(rm, na.rm = TRUE),
    SD_RM = sd(rm, na.rm = TRUE),
    Median_RM = median(rm, na.rm = TRUE),
    Mean_AGE = mean(age, na.rm = TRUE),
    SD_AGE = sd(age, na.rm = TRUE),
    Median_AGE = median(age, na.rm = TRUE),
    Mean_DIS = mean(dis, na.rm = TRUE),
    SD_DIS = sd(dis, na.rm = TRUE),
    Median_DIS = median(dis, na.rm = TRUE),
    Mean_RAD = mean(rad, na.rm = TRUE),
    SD_RAD = sd(rad, na.rm = TRUE),
    Median_RAD = median(rad, na.rm = TRUE),
    Mean_TAX = mean(tax, na.rm = TRUE),
    SD_TAX = sd(tax, na.rm = TRUE),
    Median_TAX = median(tax, na.rm = TRUE),
    Mean_PTRATIO = mean(ptratio, na.rm = TRUE),
    SD_PTRATIO = sd(ptratio, na.rm = TRUE),
    Median_PTRATIO = median(ptratio, na.rm = TRUE),
    Mean_LSTAT = mean(lstat, na.rm = TRUE),
    SD_LSTAT = sd(lstat, na.rm = TRUE),
    Median_LSTAT = median(lstat, na.rm = TRUE),
    Mean_MEDV = mean(medv, na.rm = TRUE),
    SD_MEDV = sd(medv, na.rm = TRUE),
    Median_MEDV = median(medv, na.rm = TRUE)
  ) %>%
  pivot_longer(everything(), names_to = "Statistic", values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  select(Variable, Mean, SD, Median) %>%
  mutate(Variable = case_when(
    Variable == "ZN" ~ "Proportion of residential land zoned",
    Variable == "INDUS" ~ "Proportion of non-retail business acres",
    Variable == "CHAS" ~ "Charles River dummy variable",
    Variable == "NOX" ~ "Nitrogen oxides concentration",
    Variable == "RM" ~ "Average number of rooms per dwelling",
    Variable == "AGE" ~ "Proportion of units built pre-1940",
    Variable == "DIS" ~ "Weighted distances to employment centres",
    Variable == "RAD" ~ "Accessibility to radial highways",
    Variable == "TAX" ~ "Property-tax rate per $10,000",
    Variable == "PTRATIO" ~ "Pupil-teacher ratio by town",
    Variable == "LSTAT" ~ "Lower status of the population (%)",
    Variable == "MEDV" ~ "Median value of owner-occupied homes",
    TRUE ~ Variable
  ))

summary_stats %>%
  gt() %>%
  tab_header(
    title = "Summary Statistics of Predictor Variables"
  ) %>%
  cols_label(
    Variable = "Variable",
    Mean = "Mean",
    SD = "Standard Deviation",
    Median = "Median"
  )
```

```{r}
train_dat_long = train_dat %>%
  select(-chas, -target) %>%  
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(train_dat_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Correlation Matrix

```{r}
correlation_matrix = cor(train_dat)

target_correlation = correlation_matrix["target", ]
print(target_correlation)
```

`zn` has a negative correlation with the crime rate (-0.43168176). This suggests that neighborhoods with more large-lot residential land tend to have lower crime rates. More spacious areas, possibly with fewer people packed in, might see less crime.

`indus` is positively correlated with crime (0.60485074). This one's interesting because it implies that more industrial areas tend to have higher crime rates. Maybe it's because these areas are less residential.

The `chas` variable shows a small positive correlation with crime (0.08004187). Honestly, it's so slight that it might not mean much.

`nox` has a strong positive correlation with crime (0.72610622). Areas with higher pollution levels ,also see higher crime rates. Pollution and crime are correlated.

`rm` shows a slight negative correlation with crime (-0.15255334). It suggests that neighborhoods with larger homes, tend to have slightly lower crime rates. But it's not a very strong relationship.

`age`, is positively correlated with crime (0.63010625). This suggests older neighborhoods might see more crime.

`dis`, has a negative correlation with crime (-0.61867312). The further away from employment centers, the lower the crime rate.

`rad`, and `tax` both have strong positive correlations with crime (0.62810492 and 0.61111331, respectively). More accessible areas and those with higher taxes are associated with higher crime rates. Mayve more accessible areas see more traffic, leading to more crime.

`ptratio`, shows a slight positive correlation with crime (0.25084892), suggesting that schools with more students per teacher might see slightly higher crime rates.

`lstat` is significantly positively correlated with crime (0.46912702). This mean that areas with a higher percentage of lower-status residents have higher crime rates. This makes sense because we know that socio-economic factors influence crime and most crimes are in fact just crimes of poverty.

`medv` is negatively correlated with crime (-0.27055071). Wealthier areas tend to have lower crime rates.

#### Variables to Exclude 

1.  **`zn` (-0.43168176):** While negatively correlated with crime, there is potential overlap with `indus` and `nox` in describing urban vs. suburban characteristics.
2.  **`chas` (0.08004187):** The low correlation with the crime rate suggests it may not be a strong predictor of crime.
3.  **`rm` (-0.15255334):** Given its weak negative correlation with crime, it may not add much predictive power.
4.  **`ptratio` (0.25084892):** While there's some positive correlation, itâ€™s relatively weak compared to others.
5.  **`medv` (-0.27055071):** Although negatively correlated, if choosing between `medv` and stronger predictors like `lstat` for socio-economic status, `lstat` might be the more impactful.

### 

```{r}

colSums(is.na(train_dat))
```

There is no missing data.

### Creating New Variables

-   **`tmr` (`tax`/`medv`):** This could provide a measure of the tax burden relative to property values.

-   **`rmlstat` (`rm`/`lstat`):** A higher ratio might indicate areas with more spacious living conditions but also a higher proportion of lower socio-economic status.

```{r}
train_dat = train_dat %>%
  mutate(tmr=tax/medv,
         rmlstat=rm/lstat)%>%
  select(-zn, -chas, -rm, -ptratio, -lstat, -medv)
```

## Part 2: Data Preparation

With the newly created variables, I am going reexamine the distributions.

```{r}
data_for_hist <- train_dat %>%
  select(-c(target)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(data_for_hist, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Notes:

* "age" is left-skewed
* "dis" is right-skewed
* "rmlstat" is right-skewed
* "tmr" is right-skewed

I will try a Box-Cox transformation on the variables that have positive skew:    
"tmr," "rmlstat," and "dis," as they all have a positive skew.

#### Box Cox Confidence Interval: tmr

```{r}
b = MASS::boxcox(lm(train_dat$tmr ~ 1))
```

#### Lambda Value: tmr

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would actually not be the best transformation to run on the variable
"tmr" and instead that a transformation of $\frac{1}{\sqrt x}$ would be best.

```{r}
train_dat$tmr_transformed = 1/sqrt(train_dat$tmr)
```

#### Box Cox Confidence Interval: dis

```{r}
b = MASS::boxcox(lm(train_dat$dis ~ 1))
```

#### Lambda Value: dis

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would be a good transformation to try on the variable "dis"

```{r}
train_dat$dis_transformed = log(train_dat$dis)
```

#### Box Cox Confidence Interval: rmlstat

```{r}
b = MASS::boxcox(lm(train_dat$rmlstat ~ 1))
```

#### Lambda Value: rmlstat

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would actually not be the best transformation to run on the variable
"rmlstat" and instead that a transformation of $\frac{1}{\sqrt x}$ would be best.

```{r}
train_dat$rmlstat_transformed = 1/sqrt(train_dat$rmlstat)
```

#### Checking the new distributions:

```{r}
data_for_hist <- train_dat %>%
  select(-c(target)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(data_for_hist, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

While the new transformed variables are not perfectly normal, the new transformed
variables look closer to normal.

I also want to note the distributions of the two variables "rad" and "tax." For 
both of these variables most of the distribution is concentrated on the lower end 
and there seems to be a meaningful number of observations on the higher end with
a gap in the middle of the distribution.

I want just look at the counts of the number of observations for the variable "rad"
with specific index values by increments of 5 starting with zero.

```{r}
# Calculate the count of observations in each window
window_counts <- table(cut(train_dat$rad, breaks = seq(0, max(train_dat$rad) + 5, by = 5)))

# Print the counts
print(window_counts)
```

Based on this quick window counts I realize that there is a significant gap between
high rad index and low rad index (there are no observations with values in between
a rad index values ten to twenty). I'll go ahead and create dummies that signify a rad
index value of ten or less and rad greater than twenty. I know this is not an even
split of the data and it generally matches the pattern that I see.

```{r}
train_dat$rad_leq_10 = ifelse(train_dat$rad <= 10, 1.0, 0.0)
train_dat$rad_greater_20 = ifelse(train_dat$rad > 20, 1.0, 0.0)
```

I am going to run the same analysis for tax; however this time I will make the 
increments multiples of 100.

```{r}
# Calculate the count of observations in each window
window_counts <- table(cut(train_dat$tax, breaks = seq(0, max(train_dat$tax) + 100, by = 100)))

# Print the counts
print(window_counts)
```

Now the distribution for "tax" is obviously different and it is still important to
note that we have a similar large gap. I am going to create dummies for "tax" 
observations greater than 600 and "tax" observations less than 500.

```{r}
train_dat$tax_leq_500 = ifelse(train_dat$tax <= 500, 1.0, 0.0)
train_dat$tax_greater_600 = ifelse(train_dat$tax > 600, 1.0, 0.0)
```

Both of these sets of dummies variables were created entirely base off of the 
individual distributions, there may be external knowledge which could have provided 
separate bucket thresholds; however, that is not how these variables were defined.
