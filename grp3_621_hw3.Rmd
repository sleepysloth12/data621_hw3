---
title: "Data 621 HW#3"
author: "Group 3"
date: "2024-03-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Logistic Regression of Crime Data

## Training-Data Exploration

```{r}
library(dplyr)
library(ggplot2)
library(gt)
library(tidyr)
library(MASS)
```

### Summary Stats

```{r}

train_dat=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw3/main/crime-training-data_modified.csv"))

summary_stats = train_dat %>%
  summarise(
    Mean_ZN = mean(zn, na.rm = TRUE),
    SD_ZN = sd(zn, na.rm = TRUE),
    Median_ZN = median(zn, na.rm = TRUE),
    Mean_INDUS = mean(indus, na.rm = TRUE),
    SD_INDUS = sd(indus, na.rm = TRUE),
    Median_INDUS = median(indus, na.rm = TRUE),
    Mean_CHAS = mean(chas, na.rm = TRUE),
    SD_CHAS = sd(chas, na.rm = TRUE),
    Median_CHAS = median(chas, na.rm = TRUE),
    Mean_NOX = mean(nox, na.rm = TRUE),
    SD_NOX = sd(nox, na.rm = TRUE),
    Median_NOX = median(nox, na.rm = TRUE),
    Mean_RM = mean(rm, na.rm = TRUE),
    SD_RM = sd(rm, na.rm = TRUE),
    Median_RM = median(rm, na.rm = TRUE),
    Mean_AGE = mean(age, na.rm = TRUE),
    SD_AGE = sd(age, na.rm = TRUE),
    Median_AGE = median(age, na.rm = TRUE),
    Mean_DIS = mean(dis, na.rm = TRUE),
    SD_DIS = sd(dis, na.rm = TRUE),
    Median_DIS = median(dis, na.rm = TRUE),
    Mean_RAD = mean(rad, na.rm = TRUE),
    SD_RAD = sd(rad, na.rm = TRUE),
    Median_RAD = median(rad, na.rm = TRUE),
    Mean_TAX = mean(tax, na.rm = TRUE),
    SD_TAX = sd(tax, na.rm = TRUE),
    Median_TAX = median(tax, na.rm = TRUE),
    Mean_PTRATIO = mean(ptratio, na.rm = TRUE),
    SD_PTRATIO = sd(ptratio, na.rm = TRUE),
    Median_PTRATIO = median(ptratio, na.rm = TRUE),
    Mean_LSTAT = mean(lstat, na.rm = TRUE),
    SD_LSTAT = sd(lstat, na.rm = TRUE),
    Median_LSTAT = median(lstat, na.rm = TRUE),
    Mean_MEDV = mean(medv, na.rm = TRUE),
    SD_MEDV = sd(medv, na.rm = TRUE),
    Median_MEDV = median(medv, na.rm = TRUE)
  ) %>%
  pivot_longer(everything(), names_to = "Statistic", values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  select(Variable, Mean, SD, Median) %>%
  mutate(Variable = case_when(
    Variable == "ZN" ~ "Proportion of residential land zoned",
    Variable == "INDUS" ~ "Proportion of non-retail business acres",
    Variable == "CHAS" ~ "Charles River dummy variable",
    Variable == "NOX" ~ "Nitrogen oxides concentration",
    Variable == "RM" ~ "Average number of rooms per dwelling",
    Variable == "AGE" ~ "Proportion of units built pre-1940",
    Variable == "DIS" ~ "Weighted distances to employment centres",
    Variable == "RAD" ~ "Accessibility to radial highways",
    Variable == "TAX" ~ "Property-tax rate per $10,000",
    Variable == "PTRATIO" ~ "Pupil-teacher ratio by town",
    Variable == "LSTAT" ~ "Lower status of the population (%)",
    Variable == "MEDV" ~ "Median value of owner-occupied homes",
    TRUE ~ Variable
  ))

summary_stats %>%
  gt() %>%
  tab_header(
    title = "Summary Statistics of Predictor Variables"
  ) %>%
  cols_label(
    Variable = "Variable",
    Mean = "Mean",
    SD = "Standard Deviation",
    Median = "Median"
  )
```

```{r}
train_dat_long = train_dat %>%
  select(-chas, -target) %>%  
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(train_dat_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Correlation Matrix

```{r}
correlation_matrix = cor(train_dat)

target_correlation = correlation_matrix["target", ]
print(target_correlation)
```

`zn` has a negative correlation with the crime rate (-0.43168176). This suggests that neighborhoods with more large-lot residential land tend to have lower crime rates. More spacious areas, possibly with fewer people packed in, might see less crime.

`indus` is positively correlated with crime (0.60485074). This one's interesting because it implies that more industrial areas tend to have higher crime rates. Maybe it's because these areas are less residential.

The `chas` variable shows a small positive correlation with crime (0.08004187). Honestly, it's so slight that it might not mean much.

`nox` has a strong positive correlation with crime (0.72610622). Areas with higher pollution levels ,also see higher crime rates. Pollution and crime are correlated.

`rm` shows a slight negative correlation with crime (-0.15255334). It suggests that neighborhoods with larger homes, tend to have slightly lower crime rates. But it's not a very strong relationship.

`age`, is positively correlated with crime (0.63010625). This suggests older neighborhoods might see more crime.

`dis`, has a negative correlation with crime (-0.61867312). The further away from employment centers, the lower the crime rate.

`rad`, and `tax` both have strong positive correlations with crime (0.62810492 and 0.61111331, respectively). More accessible areas and those with higher taxes are associated with higher crime rates. Mayve more accessible areas see more traffic, leading to more crime.

`ptratio`, shows a slight positive correlation with crime (0.25084892), suggesting that schools with more students per teacher might see slightly higher crime rates.

`lstat` is significantly positively correlated with crime (0.46912702). This mean that areas with a higher percentage of lower-status residents have higher crime rates. This makes sense because we know that socio-economic factors influence crime and most crimes are in fact just crimes of poverty.

`medv` is negatively correlated with crime (-0.27055071). Wealthier areas tend to have lower crime rates.

#### Variables to Exclude 

1.  **`zn` (-0.43168176):** While negatively correlated with crime, there is potential overlap with `indus` and `nox` in describing urban vs. suburban characteristics.
2.  **`chas` (0.08004187):** The low correlation with the crime rate suggests it may not be a strong predictor of crime.
3.  **`rm` (-0.15255334):** Given its weak negative correlation with crime, it may not add much predictive power.
4.  **`ptratio` (0.25084892):** While there's some positive correlation, itâ€™s relatively weak compared to others.
5.  **`medv` (-0.27055071):** Although negatively correlated, if choosing between `medv` and stronger predictors like `lstat` for socio-economic status, `lstat` might be the more impactful.

### 

```{r}

colSums(is.na(train_dat))
```

There is no missing data.

### Creating New Variables

-   **`tmr` (`tax`/`medv`):** This could provide a measure of the tax burden relative to property values.

-   **`rmlstat` (`rm`/`lstat`):** A higher ratio might indicate areas with more spacious living conditions but also a higher proportion of lower socio-economic status.

```{r}
train_dat = train_dat %>%
  mutate(tmr=tax/medv,
         rmlstat=rm/lstat)%>%
  select(-zn, -chas, -rm, -ptratio, -lstat, -medv)
```

## Part 2: Data Preparation

With the newly created variables, I am going reexamine the distributions.

```{r}
data_for_hist <- train_dat %>%
  select(-c(target)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(data_for_hist, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Notes:

* "age" is left-skewed
* "dis" is right-skewed
* "rmlstat" is right-skewed
* "tmr" is right-skewed

I will try a Box-Cox transformation on the variables that have positive skew:    
"tmr," "rmlstat," and "dis," as they all have a positive skew.

#### Box Cox Confidence Interval: tmr

```{r}
b = MASS::boxcox(lm(train_dat$tmr ~ 1))
```

#### Lambda Value: tmr

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would actually not be the best transformation to run on the variable
"tmr" and instead that a transformation of $\frac{1}{\sqrt x}$ would be best.

```{r}
train_dat$tmr_transformed = 1/sqrt(train_dat$tmr)
```

#### Box Cox Confidence Interval: dis

```{r}
b = MASS::boxcox(lm(train_dat$dis ~ 1))
```

#### Lambda Value: dis

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would be a good transformation to try on the variable "dis"

```{r}
train_dat$dis_transformed = log(train_dat$dis)
```

#### Box Cox Confidence Interval: rmlstat

```{r}
b = MASS::boxcox(lm(train_dat$rmlstat ~ 1))
```

#### Lambda Value: rmlstat

```{r}
lambda = b$x[which.max(b$y)]
print(lambda)
```

This box cox diagnostic test is interesting because it suggests that a log 
transformation would actually not be the best transformation to run on the variable
"rmlstat" and instead that a transformation of $\frac{1}{\sqrt x}$ would be best.

```{r}
train_dat$rmlstat_transformed = 1/sqrt(train_dat$rmlstat)
```

#### Checking the new distributions:

```{r}
data_for_hist <- train_dat %>%
  select(-c(target)) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(data_for_hist, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

While the new transformed variables are not perfectly normal, the new transformed
variables look closer to normal.

I also want to note the distributions of the two variables "rad" and "tax." For 
both of these variables most of the distribution is concentrated on the lower end 
and there seems to be a meaningful number of observations on the higher end with
a gap in the middle of the distribution.

I want just look at the counts of the number of observations for the variable "rad"
with specific index values by increments of 5 starting with zero.

```{r}
# Calculate the count of observations in each window
window_counts <- table(cut(train_dat$rad, breaks = seq(0, max(train_dat$rad) + 5, by = 5)))

# Print the counts
print(window_counts)
```

Based on this quick window counts I realize that there is a significant gap between
high rad index and low rad index (there are no observations with values in between
a rad index values ten to twenty). I'll go ahead and create dummies that signify a rad
index value of ten or less and rad greater than twenty. I know this is not an even
split of the data and it generally matches the pattern that I see.

```{r}
train_dat$rad_leq_10 = ifelse(train_dat$rad <= 10, 1.0, 0.0)
train_dat$rad_greater_20 = ifelse(train_dat$rad > 20, 1.0, 0.0)
```

I am going to run the same analysis for tax; however this time I will make the 
increments multiples of 100.

```{r}
# Calculate the count of observations in each window
window_counts <- table(cut(train_dat$tax, breaks = seq(0, max(train_dat$tax) + 100, by = 100)))

# Print the counts
print(window_counts)
```

Now the distribution for "tax" is obviously different and it is still important to
note that we have a similar large gap. I am going to create dummies for "tax" 
observations greater than 600 and "tax" observations less than 500.

```{r}
train_dat$tax_leq_500 = ifelse(train_dat$tax <= 500, 1.0, 0.0)
train_dat$tax_greater_600 = ifelse(train_dat$tax > 600, 1.0, 0.0)
```

Both of these sets of dummies variables were created entirely base off of the 
individual distributions, there may be external knowledge which could have provided 
separate bucket thresholds; however, that is not how these variables were defined.

Now that we have explored these data, let's move on to building appropriate logistic regression models. To start, I will fit a full regression model, and use AIC criteria to determine some of the best fitting models. First, however, I will separate the dataset by the transformed and non-transformed variables, in order to more easily partition the models 

```{r}
train_orig <- subset(train_dat, select = -c(tmr_transformed:tax_greater_600))
train_transform <- subset(train_dat, select = -c(dis, tmr, rmlstat, rad_leq_10:tax_greater_600))
train_dummy <- subset(train_dat, select = -c(rad, tax, dis, tmr, rmlstat))


full_model_orig <- glm(target ~ ., data = train_orig, family = binomial)
```

Now, let's create some models and see which would produce the best fitting models based on AIC stepwise algorithms. I decided to go this route because stepwise models are able to quickly and efficiently generate reliable regression models based on generating the lowest possible AIC criteria. For these models, I will have the alogrithms perform both forward selection and backwards elimination.

```{r}
full_model_orig <- glm(target ~ ., data = train_orig, family = binomial)

summary(full_model_orig)
full_model_trans <- glm(target ~ ., data = train_transform, family = binomial)
full_model_dummy <- glm(target ~ ., data = train_dummy, family = binomial)

set.seed(12345)
step_orig <- stepAIC(full_model_orig, direction = c("both"), trace = F)
summary(step_orig)

step_trans <- stepAIC(full_model_trans, direction = c("both"), trace = F)
summary(step_trans)

step_dummy <- stepAIC(full_model_dummy, direction = c("both"), trace = F)
summary(step_dummy)
```

Based on the stepwise algorithms produced, most models were left with around 5 or 6 predictors. Our first model (using the original, non-transformed variables) demonstrated that nitrogen oxide concentration, the proportion of older occupied units, and the accessibility of radial highways significantly positively predicted an increased log likelihood  of crime occurring. However, increases in taxation significantly predicted decreases in the likelihood of crime. Distance to employment centers and the likelihood of more spacious living conditions did not significantly predict changes in crime likelihood.

Our second model indicated that nitrogen oxide, the proportion of older occupied units, accessibility of radial highways, and distances from employment centers significantly predicted increases in the likelihood of crime. Similar to our first model, increases in taxation predicted decreases in the likelihood of crime. The result for distance is contrary to our first model, indicating that the normalization procedure employed may have had an effect on the interpretability of that variable within the context of these general linear models.

Finally, our third model only showed significant effects for nitrogen oxide, the proportion of older occupied units, and distance from employment centers. Similar to the first model, the likelihood of living in a spacious living condition did not have a significant effect on crime likelihood. In addition, the dummy variables that were created did not have a significant effect.

Overall, based on the models above, we can consistently see that the presence of nitrogen oxide, increases in the proportion of older occupied units, distance from employment centers, and the accessibility of radial highways all significantly predict increases in the likelihood of crime, whereas increases in taxation predict decreases in the likelihood. 

I believe these results make sense within the context of this dataset. By itself, the presence of older housing units may not be able to explain changes in crime, but it may make sense if we consider the prices of those units, and the average income of the areas in which those domiciles are located. The index of radial highways may indicate an increase in the volume of people passing through a specific area; an increased density of individuals may correspond with increased crime rates. Distance from employment centers may suggest a larger proportion of unemployed individuals within an area, which may lead to increases in crime as a way to alleviate financial burdens; alternatively, this may suggest these areas are lower income in nature, and may have less access to resources and security. Increases in taxation likely suggest regions that are higher income and possess regular access to resources and police precincts that may deter the presence of crime within those areas. Seeing the presence of nitrogen oxide corresponding with the likelihood of crime was interesting; however, some research has indicated that exposure to pollution can lead to increased violent behavior. Alternatively, perhaps exposure to nitrogen oxide can have other sociological implications that predict increases in the likelihood of crime.

While these results do seem to make sense, it is very likely that there are underlying mediating and moderating factors that may better explain these predictions. For instance, it may help to include variables related to access to community resources, education, and security as predictors. These variables are much more likely to offer direct explanations for the trends described in this dataset.

Notes for model selection:
- Lower AIC tends to be better
- Higher Null deviance is better
- Lower Residual Deviance is better

